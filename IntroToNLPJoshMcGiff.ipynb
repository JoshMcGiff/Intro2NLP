{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steam/Metacritic Review Analysis - by Josh McGiff\n",
    "## Introduction\n",
    "\n",
    "This Jupyter notebook is designed for a lecture that guides students through the analysis of Steam game reviews using Natural Language Processing (NLP) techniques. The notebook covers various steps involved in the analysis, including data retrieval, Named Entity Recognition (NER), visualisation of entities, and exploring features of reviews through NLP methods. Additionally, it introduces the use of machine learning models for sentiment analysis and sexism detection in reviews.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving Review Data\n",
    "\n",
    "To begin the analysis, we need to retrieve review data from the Steam platform using their API. We'll define functions to fetch reviews based on an AppID and filter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def get_review_data(app_id, params={'json': 1}):\n",
    "    base_url = 'https://store.steampowered.com/appreviews/'\n",
    "    response = requests.get(url=base_url + str(app_id), params=params)\n",
    "    return response.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_n_reviews(app_id, filter, n=1000):\n",
    "    reviews = []\n",
    "    cursor = '*'\n",
    "    base_url = 'https://store.steampowered.com/appreviews/'\n",
    "\n",
    "    while n > 0:\n",
    "        params = {\n",
    "            'json': 1,\n",
    "            'filter': filter,  \n",
    "            'language': 'english',\n",
    "            'day_range': 9223372036854775807,\n",
    "            'review_type': 'all',\n",
    "            'purchase_type': 'all',\n",
    "            'cursor': cursor.encode(),\n",
    "            'num_per_page': min(100, n)\n",
    "        }\n",
    "\n",
    "        response = requests.get(url=base_url + str(app_id), params=params)\n",
    "        data = response.json()\n",
    "\n",
    "        cursor = data['cursor']\n",
    "        reviews += data['reviews']\n",
    "        n -= min(100, n)\n",
    "\n",
    "        if len(data['reviews']) < 100:\n",
    "            break\n",
    "\n",
    "    return reviews\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”ŽChoose a Steam product to analyse by searching the platform. Once a selection is made, extract the AppID from the URL of the page:\n",
    "- The URL format is: https://store.steampowered.com/app/[EXTRACT THIS APPID]/GameTitle/\n",
    "- eg: 292120 from https://store.steampowered.com/app/292120/FINAL_FANTASY_XIII/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_id = 1635590\n",
    "\n",
    "reviews = []\n",
    "reviews += (get_n_reviews(app_id, 'all'))\n",
    "print(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition (NER)\n",
    "\n",
    "Before diving into the analysis, it's essential to understand the entities mentioned in the reviews. We'll use spaCy's NER model to extract named entities from the reviews and visualize them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Install required Named Entity Recognition model from the spaCy library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only need to run once!:\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the named entities from the reviews and output them to [app_id].csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "# Extract the review text and the corresponding upvote status\n",
    "df = pd.DataFrame(reviews)[['review', 'voted_up']]\n",
    "\n",
    "# Load the English language model for spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Function to clean text: removes non-alphanumeric characters and extra whitespaces\n",
    "def clean_text(text):\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # Remove non-alphanumeric characters\n",
    "    cleaned_text = ' '.join(cleaned_text.split())      # Remove extra whitespaces including newlines\n",
    "    return cleaned_text.strip()\n",
    "\n",
    "# Clean the text in the 'review' column and apply Named Entity Recognition (NER)\n",
    "def process_review(review):\n",
    "    processed_review = clean_text(review)\n",
    "    doc = nlp(processed_review.strip())                 # Strip to remove leading/trailing whitespaces\n",
    "    entities = [{'Entity': ent.text, 'Label': ent.label_} for ent in doc.ents]\n",
    "    return processed_review, entities\n",
    "\n",
    "# Apply text cleaning and NER to each review in the DataFrame\n",
    "df['review'], df['Entities'] = zip(*df['review'].apply(process_review))\n",
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "csv_filename = str(app_id) + '.csv'\n",
    "df.to_csv(csv_filename, index=False)\n",
    "\n",
    "print('DataFrame with entities saved to', csv_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise the most frequently occuring entities with a barchart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the DataFrame containing the entities\n",
    "df = pd.read_csv(csv_filename)\n",
    "\n",
    "# Flatten the list of entities into a single list\n",
    "all_entities = [entity['Entity'] for entities in df['Entities'] for entity in eval(entities)]\n",
    "\n",
    "# Create a DataFrame to count the occurrences of each entity\n",
    "entity_counts = pd.Series(all_entities).value_counts().reset_index()\n",
    "entity_counts.columns = ['Entity', 'Count']\n",
    "\n",
    "# Plot the top N most common entities\n",
    "top_n = 10  # Change this value to visualize more or fewer entities\n",
    "top_entities = entity_counts.head(top_n)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Count', y='Entity', data=top_entities, palette='viridis')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Entity')\n",
    "plt.title(f'Top {top_n} Most Common Entities')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise the most frequently occuring entities with a wordcloud:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# Convert the list of entities into a single string\n",
    "all_entities_text = ' '.join(all_entities)\n",
    "\n",
    "# Generate a word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_entities_text)\n",
    "\n",
    "# Plot the word cloud\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Entity Word Cloud')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise the most frequently occuring entities types with a stacked barchart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the DataFrame containing entities\n",
    "df = pd.read_csv(csv_filename)\n",
    "\n",
    "# Flatten the list of entities and labels into separate lists\n",
    "all_entities = [entity['Entity'] for entities in df['Entities'] for entity in eval(entities)]\n",
    "all_labels = [entity['Label'] for entities in df['Entities'] for entity in eval(entities)]\n",
    "\n",
    "# Count the occurrences of each label\n",
    "label_counts = pd.Series(all_labels).value_counts()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "label_counts.plot(kind='bar', stacked=True)\n",
    "plt.xlabel('Entity Label')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Stacked Bar Plot of Entity Labels')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis solely using NER only provides a limited view of inference. Contextually informative text such as adjectives are highly valuable when building a bigger picture of the perception of a product via analysing reviews.**\n",
    "\n",
    "**Therefore, let's try to investigate various features of the reviews:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(csv_filename)\n",
    "\n",
    "# Convert the 'review' column to strings\n",
    "df['review'] = df['review'].astype(str)\n",
    "\n",
    "# Combine all the reviews into a single string\n",
    "combined_reviews = ' '.join(df['review'])\n",
    "\n",
    "# Generate the word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(combined_reviews)\n",
    "\n",
    "# Display the word cloud using matplotlib\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import FreqDist, ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Download the punkt resource\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(combined_reviews)\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "# Generate bigrams\n",
    "bigrams = list(ngrams(filtered_tokens, 2))\n",
    "\n",
    "# Filter bigrams that contain specific terms like 'main character' or 'protagonist'\n",
    "specific_term_bigrams = [bigram for bigram in bigrams]\n",
    "\n",
    "# Calculate frequency distribution of specific term-related bigrams\n",
    "specific_term_bigram_freq = FreqDist(specific_term_bigrams)\n",
    "\n",
    "# Extract the most common bigrams related to the specific terms\n",
    "most_common_specific_term_bigrams = specific_term_bigram_freq.most_common(20)\n",
    "\n",
    "# Extract bigrams and their frequencies\n",
    "bigram, freq = zip(*most_common_specific_term_bigrams)\n",
    "\n",
    "# Plot the bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(bigram)), freq, color='skyblue')\n",
    "plt.yticks(range(len(bigram)), [' '.join(b) for b in bigram])  # Set bigram labels\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Bigram')\n",
    "plt.title('Most Frequently Occurring Bigrams')\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to display the most frequent bigrams at the top\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import FreqDist, ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(combined_reviews)\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "# Generate trigrams\n",
    "trigrams = list(ngrams(filtered_tokens, 3))\n",
    "\n",
    "# Filter trigrams that contain specific terms like 'main character' or 'protagonist'\n",
    "specific_term_trigrams = [trigram for trigram in trigrams]\n",
    "\n",
    "# Calculate frequency distribution of specific term-related trigrams\n",
    "specific_term_trigram_freq = FreqDist(specific_term_trigrams)\n",
    "\n",
    "# Extract the most common trigrams related to the specific terms\n",
    "most_common_specific_term_trigrams = specific_term_trigram_freq.most_common(20)\n",
    "\n",
    "# Extract trigrams and their frequencies\n",
    "trigram, freq = zip(*most_common_specific_term_trigrams)\n",
    "\n",
    "# Plot the bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(trigram)), freq, color='skyblue')\n",
    "plt.yticks(range(len(trigram)), [' '.join(t) for t in trigram])  # Set trigram labels\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Trigram')\n",
    "plt.title('Most Frequently Occurring Trigrams')\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to display the most frequent trigrams at the top\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**While this analysis of all the reviews may yield insights into the general perception of the chosen product, it is possible that important information could be hidden due to a lack of representative data.**\n",
    "\n",
    "**Imagine that you are the developer behind the product that is being reviewed, and the game has 80% positive reviews and 20% negative reviews. Selecting a subset that contains all the negative reviews could help to highlight the reasons for the poor reception. This could enable developers to fix bug-related issues or adopt product-specific that could improve future products.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_id = 1635590\n",
    "\n",
    "reviews = []\n",
    "reviews += (get_n_reviews(app_id, 'negative'))\n",
    "print(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "# Extract the review text and the corresponding upvote status\n",
    "df = pd.DataFrame(reviews)[['review', 'voted_up']]\n",
    "\n",
    "# Load the English language model for spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Function to clean text: removes non-alphanumeric characters and extra whitespaces\n",
    "def clean_text(text):\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # Remove non-alphanumeric characters\n",
    "    cleaned_text = ' '.join(cleaned_text.split())      # Remove extra whitespaces including newlines\n",
    "    return cleaned_text.strip()\n",
    "\n",
    "# Clean the text in the 'review' column and apply Named Entity Recognition (NER)\n",
    "def process_review(review):\n",
    "    processed_review = clean_text(review)\n",
    "    doc = nlp(processed_review.strip())                 # Strip to remove leading/trailing whitespaces\n",
    "    entities = [{'Entity': ent.text, 'Label': ent.label_} for ent in doc.ents]\n",
    "    return processed_review, entities\n",
    "\n",
    "# Apply text cleaning and NER to each review in the DataFrame\n",
    "df['review'], df['Entities'] = zip(*df['review'].apply(process_review))\n",
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "csv_filename = str(app_id) + '.csv'\n",
    "df.to_csv(csv_filename, index=False)\n",
    "\n",
    "print('DataFrame with entities saved to', csv_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise the most frequently occuring entities with a barchart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the DataFrame containing the entities\n",
    "df = pd.read_csv(csv_filename)\n",
    "\n",
    "# Flatten the list of entities into a single list\n",
    "all_entities = [entity['Entity'] for entities in df['Entities'] for entity in eval(entities)]\n",
    "\n",
    "# Create a DataFrame to count the occurrences of each entity\n",
    "entity_counts = pd.Series(all_entities).value_counts().reset_index()\n",
    "entity_counts.columns = ['Entity', 'Count']\n",
    "\n",
    "# Plot the top N most common entities\n",
    "top_n = 10  # Change this value to visualise more or fewer entities\n",
    "top_entities = entity_counts.head(top_n)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Count', y='Entity', data=top_entities, palette='viridis')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Entity')\n",
    "plt.title(f'Top {top_n} Most Common Entities')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise the most frequently occuring entities with a wordcloud:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# Convert the list of entities into a single string\n",
    "all_entities_text = ' '.join(all_entities)\n",
    "\n",
    "# Generate a word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_entities_text)\n",
    "\n",
    "# Plot the word cloud\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Entity Word Cloud')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise the most frequently occuring entities types with a stacked barchart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the DataFrame containing entities\n",
    "df = pd.read_csv(csv_filename)\n",
    "\n",
    "# Flatten the list of entities and labels into separate lists\n",
    "all_entities = [entity['Entity'] for entities in df['Entities'] for entity in eval(entities)]\n",
    "all_labels = [entity['Label'] for entities in df['Entities'] for entity in eval(entities)]\n",
    "\n",
    "# Count the occurrences of each label\n",
    "label_counts = pd.Series(all_labels).value_counts()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "label_counts.plot(kind='bar', stacked=True)\n",
    "plt.xlabel('Entity Label')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Stacked Bar Plot of Entity Labels')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis solely using NER only provides a limited view of inference. Contextually informative text such as adjectives are highly valuable when building a bigger picture of the perception of a product via analysing reviews.\n",
    "\n",
    "Therefore, let's try to investigate various features of the reviews:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Review Features\n",
    "\n",
    "Understanding various features of reviews beyond just entities is crucial. We'll explore features like word frequency, bigrams, and trigrams to gain deeper insights into the reviews.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(csv_filename)\n",
    "\n",
    "# Convert the 'review' column to strings\n",
    "df['review'] = df['review'].astype(str)\n",
    "\n",
    "# Combine all the reviews into a single string\n",
    "combined_reviews = ' '.join(df['review'])\n",
    "\n",
    "# Generate the word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(combined_reviews)\n",
    "\n",
    "# Display the word cloud using matplotlib\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import FreqDist, ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Download the punkt resource\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(combined_reviews)\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "# Generate bigrams\n",
    "bigrams = list(ngrams(filtered_tokens, 2))\n",
    "\n",
    "# Filter bigrams that contain specific terms like 'main character' or 'protagonist'\n",
    "specific_term_bigrams = [bigram for bigram in bigrams]\n",
    "\n",
    "# Calculate frequency distribution of specific term-related bigrams\n",
    "specific_term_bigram_freq = FreqDist(specific_term_bigrams)\n",
    "\n",
    "# Extract the most common bigrams related to the specific terms\n",
    "most_common_specific_term_bigrams = specific_term_bigram_freq.most_common(20)\n",
    "\n",
    "# Extract bigrams and their frequencies\n",
    "bigram, freq = zip(*most_common_specific_term_bigrams)\n",
    "\n",
    "# Plot the bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(bigram)), freq, color='skyblue')\n",
    "plt.yticks(range(len(bigram)), [' '.join(b) for b in bigram])  # Set bigram labels\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Bigram')\n",
    "plt.title('Most Frequently Occurring Bigrams')\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to display the most frequent bigrams at the top\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import FreqDist, ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(combined_reviews)\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "# Generate trigrams\n",
    "trigrams = list(ngrams(filtered_tokens, 3))\n",
    "\n",
    "# Filter trigrams that contain specific terms like 'main character' or 'protagonist'\n",
    "specific_term_trigrams = [trigram for trigram in trigrams]\n",
    "\n",
    "# Calculate frequency distribution of specific term-related trigrams\n",
    "specific_term_trigram_freq = FreqDist(specific_term_trigrams)\n",
    "\n",
    "# Extract the most common trigrams related to the specific terms\n",
    "most_common_specific_term_trigrams = specific_term_trigram_freq.most_common(20)\n",
    "\n",
    "# Extract trigrams and their frequencies\n",
    "trigram, freq = zip(*most_common_specific_term_trigrams)\n",
    "\n",
    "# Plot the bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(trigram)), freq, color='skyblue')\n",
    "plt.yticks(range(len(trigram)), [' '.join(t) for t in trigram])  # Set trigram labels\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Trigram')\n",
    "plt.title('Most Frequently Occurring Trigrams')\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to display the most frequent trigrams at the top\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸŒŸ Bonus section: Sentiment Analysis\n",
    "\n",
    "Sentiment analysis helps in understanding the overall sentiment expressed in reviews. We'll utilise a pre-trained sentiment analysis model to classify reviews as positive or negative.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The previous sections utilise a wide-range of NLP/text analysis methods. However, they distinctly lack many of the more frequently used machine learning methods such as sentiment analysis, text summarisation and machine translation.**\n",
    "\n",
    "What if we wanted to compare the rating (i.e positive/negative) of a review with the results of a sentiment analysis model? Is it possible that reviewers gave a positive recommendation, even if their written review was predominantly negative?\n",
    "\n",
    "What if we wanted to investigate the amount of reviews that engage in hate speech? Movies, games and shows are frequently \"review-bombed\" by masses of users due to discriminatory views. Filtering your analysis of reviews to exclude discriminatory content could provide more genuine & informative feedback on the product. \n",
    "\n",
    "Interesting & concerning links:\n",
    "^ \"How Captain Marvel and Brie Larson beat the internetâ€™s sexist trolls\" - https://www.vox.com/culture/2019/3/8/18254584/captain-marvel-boycott-controversy\n",
    "^ \"Marvel's Eternals Gets Review Bombed for LGBTQ+ Relationship\" - https://thedirect.com/article/marvel-eternals-reviews-bombed-lgbtq-relationship\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a proof on concept, let's use a sexism detection model from HuggingFace. https://huggingface.co/NLP-LTU/bertweet-large-sexism-detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules from the transformers library\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\n",
    "\n",
    "# Import the PyTorch library\n",
    "import torch\n",
    "\n",
    "# Load the pre-trained model for sequence classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained('NLP-LTU/distilbert-sexism-detector')\n",
    "\n",
    "# Load the tokenizer associated with the pre-trained model\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Create a text classification pipeline using the pre-trained model and tokenizer\n",
    "classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, max_len=512):\n",
    "    # Tokenize the text\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=True)\n",
    "    \n",
    "    # Truncate or pad to the desired length\n",
    "    tokens = tokens[:max_len]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(csv_filename)\n",
    "\n",
    "# Assuming your CSV file has a column named 'review' containing the reviews\n",
    "reviews = df['review'].tolist()\n",
    "\n",
    "# Preprocess each review in the list\n",
    "max_len = 511\n",
    "processed_reviews = [preprocess_text(review, max_len) for review in reviews]\n",
    "\n",
    "# Assuming you have a tokenizer initialised somewhere in your code\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Decode the processed reviews\n",
    "decoded_reviews = [tokenizer.decode(tokens, skip_special_tokens=True) for tokens in processed_reviews]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the classifier to each review in the list\n",
    "predictions = [classifier(review) for review in decoded_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise an empty list to store reviews identified as sexist\n",
    "sexistReviews = []\n",
    "\n",
    "# Iterate through each review and its corresponding prediction\n",
    "for review, prediction in zip(reviews, predictions):\n",
    "    # Check if the predicted label for the review is \"sexist\"\n",
    "    if prediction[0][\"label\"] == \"sexist\":\n",
    "        # Print the review\n",
    "        print(f\"Review: {review}\")\n",
    "        # Append the review to the list of sexist reviews\n",
    "        sexistReviews.append(review)\n",
    "\n",
    "# Print the list of sexist reviews\n",
    "print(sexistReviews)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Through this notebook, students will gain hands-on experience in analysing text data, extracting insights, and applying machine learning models to understand user sentiments and identify problematic content in reviews.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
